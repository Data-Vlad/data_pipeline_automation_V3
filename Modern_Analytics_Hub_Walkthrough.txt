================================================================================
          MODERN ANALYTICS & AI HUB: THE ULTIMATE DEMO GUIDE
================================================================================

This is a comprehensive, step-by-step script for demonstrating the full power of 
the Modern Analytics & AI Hub. It is designed to "show off" the system's 
capabilities, moving from descriptive analytics to advanced AI and self-healing 
data pipelines.

--------------------------------------------------------------------------------
PHASE 1: ENVIRONMENT SETUP
--------------------------------------------------------------------------------

1.  **Install Dependencies & Drivers**
    Open your terminal in the project root and run:
    > pip install pandas sqlalchemy pyodbc openai plotly streamlit scipy scikit-learn faker
    *Ensure you have the ODBC Driver 17 for SQL Server installed.*

2.  **Configure Environment Variables**
    Create or edit the `.env` file in the project root:
    - DB_SERVER=...
    - DB_DATABASE=...
    - DB_USERNAME=...
    - DB_PASSWORD=...
    - OPENAI_API_KEY=sk-... (Crucial for Text-to-SQL, Semantic Search, and Vision)

3.  **Generate the "Perfect" Demo Data**
    Run the setup script. This doesn't just create tables; it injects specific
    statistical patterns, anomalies, and text correlations that the AI is 
    programmed to find later.
    > python scripts/setup_demo_environment.py
    
    *Check Output*: Look for "ðŸŽ‰ Demo Environment Setup Complete!".

--------------------------------------------------------------------------------
PHASE 2: GENERATE AI INSIGHTS (BATCH LAYER)
--------------------------------------------------------------------------------

Before the demo, we need the "Batch AI" to process history, train models, and 
detect the anomalies we injected.

1.  Start Dagster:
    > dagster dev

2.  Open the Dagster UI (http://localhost:3000).
3.  Locate the asset group `analytics_ai` (or search for `run_predictive_analytics`).
4.  Click **Materialize** on the `run_predictive_analytics` asset.
    *   *Behind the Scenes*: The system pulls the last year of data, trains an 
        Isolation Forest model to find outliers, runs a Prophet forecast for the 
        next 30 days, and saves all predictions to the SQL database.

--------------------------------------------------------------------------------
PHASE 3: THE "SHOW OFF" DEMO SCRIPT (UI LAYER)
--------------------------------------------------------------------------------

Launch the Analytics UI in a new terminal:
> streamlit run analytics_ui.py

**Login**: Username: `admin` / Password: `admin123`

================================================================================
SCENARIO 1: THE EXECUTIVE OVERVIEW (Generative AI / LLM)
================================================================================
*Goal: Show that the system understands data context without human input.*

1.  **Navigate to "Dashboard"**.
2.  **Point out the Metrics**:
    - "AI Detected Anomalies": Point out the red number (e.g., 5).
    - "Failed Runs": Point out the system health.
3.  **The "Wow" Factor**: Scroll to **Automated Data Story**.
    - *Action*: Read the first bullet point aloud.
    - *Talking Point*: "I didn't write this report. A Generative AI (LLM) agent analyzed the raw pipeline logs and anomaly tables to generate this executive summary in real-time."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~112-117)
        - Logic: `ml_engine.py` (Lines ~198-226, `generate_data_story`)

================================================================================
SCENARIO 2: CONVERSATIONAL ANALYTICS (LLM-Powered Text-to-SQL)
================================================================================
*Goal: Show democratization of data access.*

1.  **Navigate to "Conversational Analytics"**.
2.  **The Query**: Type: `Show me total sales by region for last month`
3.  **The Result**:
    - Show the generated SQL code block (proving transparency/security).
    - Show the resulting data table.
    - *Talking Point*: "This uses an LLM to translate natural language into SQL. It understands 'last month' and 'sales', writes valid SQL, checks it for safety, and executes it."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~153-167)
        - Logic: `ml_engine.py` (Lines ~238-257, `generate_sql_from_question`)

================================================================================
SCENARIO 3: AGENTIC ANALYST (Autonomous AI Agent)
================================================================================
*Goal: Show the AI planning and executing a complex task autonomously.*

1.  **Navigate to "Agentic Analyst"**.
2.  **The Goal**: Enter a high-level goal:
    `Analyze the sales trend for the last 3 months, identify any significant drops, and check customer feedback for those periods to explain why.`
3.  **Action**: Click **Launch Agent**.
4.  **The Reveal**:
    - **Planning (Chain-of-Thought)**: Show the "Agent is planning..." expander. It breaks the goal into steps (Data Retrieval -> Analysis).
        - *Learning Point*: This simulates "Chain of Thought" reasoning, where the LLM breaks a complex problem into sub-tasks before acting.
    - **Execution (Tool Use)**:
        - **Tool 1 (SQL Generator)**: It generates SQL to get sales data.
        - **Tool 2 (Data Analyzer)**: It calculates metrics from the retrieved dataframe.
        - **Tool 3 (Storyteller)**: It generates a final report explaining the drop (referencing the "server error" feedback injected in the demo data).
    - *Talking Point*: "This isn't just a chatbot. It's an agent. It formulated a plan, queried the database, found the anomaly, and then correlated it with customer feedback to give me a complete answer."
    - *Code Reference*:
        - UI Implementation: `analytics_ui.py` (Lines ~185-246)
            - *Planning Simulation*: Lines ~201-207
            - *Context Retrieval*: Lines ~212-214
            - *Tool Execution*: Lines ~217-240
        - AI Logic: `ml_engine.py`
            - *Text-to-SQL Tool*: Lines ~238-257 (`generate_sql_from_question`)
            - *Insight Generator Tool*: Lines ~198-226 (`generate_data_story`)

================================================================================
SCENARIO 4: DIAGNOSTIC AI (Root Cause Analysis)
================================================================================
*Goal: Solve a business mystery in seconds.*

1.  **Navigate to "Root Cause Analysis"**.
2.  **Setup**:
    - Table: `fact_retail_sales`
    - Metric: `SalesAmount`
    - Target Date: Select the date **3 days ago** (The setup script injected a 80% drop here).
3.  **Action**: Click **Analyze Drivers**.
4.  **The Reveal**:
    - The system will display `Region = North` and `Category = Electronics` with a large negative impact (e.g., -80%).
    - *Talking Point*: "Usually, an analyst would spend hours slicing data in Excel to find this. The AI scanned every combination of dimensions and found the exact source of the crash: Electronics in the North region."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~261)
        - Logic: `ml_engine.py` (Lines ~60-115, `analyze_root_cause`)

================================================================================
SCENARIO 5: SEMANTIC INTELLIGENCE (LLM RAG / Vector Search)
================================================================================
*Goal: Connect the quantitative drop to qualitative feedback.*

1.  **Navigate to "Semantic Search"**.
2.  **Context**: "We know sales dropped in North/Electronics. Was it a pricing issue? A product issue?"
3.  **Setup**:
    - Table: `fact_retail_sales`
    - Column: `CustomerFeedback`
    - Query: `technical issues during checkout`
4.  **Action**: Click **Search**.
5.  **The Reveal**:
    - The results show rows containing: *"Cannot checkout, server error"*.
    - *Crucial Point*: Note that the word "technical" might not appear in the result.
    - *Talking Point*: "This uses LLM Embeddings for Semantic Search. I searched for 'technical issues', and it found 'server error' because the LLM understands they mean the same thing. Traditional keyword search would have failed here."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~351)
        - Logic: `ml_engine.py` (Lines ~359-395, `perform_semantic_search`)

================================================================================
SCENARIO 6: PRESCRIPTIVE ANALYTICS (Optimization)
================================================================================
*Goal: Move from "What happened?" to "What should we do?".*

1.  **Navigate to "Prescriptive Optimization"**.
2.  **Context**: "We need to recover that lost revenue next week. How much should we spend on ads?"
3.  **Setup**:
    - Table: `fact_retail_sales`
    - Target: `SalesAmount`
    - Inputs: Select `MarketingSpend` and `DiscountRate`.
4.  **Action**: Click **Run Optimization**.
5.  **The Reveal**:
    - The system displays a "Projected Maximum Target".
    - It gives specific recommendations: e.g., *MarketingSpend: $840.50*, *DiscountRate: 0.15*.
    - *Talking Point*: "The system modeled the elasticity of demand based on our history and used a mathematical solver to give us the optimal marketing mix."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~320)
        - Logic: `ml_engine.py` (Lines ~306-357, `optimize_business_objective`)

================================================================================
SCENARIO 7: MULTI-MODAL ANALYSIS (Vision LLM)
================================================================================
*Goal: Show the system handling unstructured, non-text data.*

1.  **Navigate to "Multi-Modal Analysis"**.
2.  **Context**: "We just received a vendor invoice as a scanned image."
3.  **Action**:
    - Upload a sample image (e.g., a screenshot of an invoice or a receipt).
    - In "Fields to Extract", type: `Invoice Number, Total Amount, Vendor Name, Date`.
    - Click **Extract Data**.
4.  **The Reveal**:
    - The JSON result appears with the correct data extracted from the pixels.
    - *Talking Point*: "We are using a Multi-Modal LLM (GPT-4 Vision) to turn pixels into structured database records instantly."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~406)
        - Logic: `ml_engine.py` (Lines ~432-483, `extract_structured_data`)

================================================================================
SCENARIO 8: AUTONOMOUS DATA REPAIR (Self-Healing)
================================================================================
*Goal: Show AI maintaining data quality.*

1.  **Navigate to "Autonomous Data Repair"**.
2.  **Context**: "Data entry errors happen. Let's see if the AI can find them."
3.  **Setup**:
    - Table: `fact_retail_sales`
4.  **Action**: Click **Scan for Issues**.
5.  **The Reveal**:
    - The system should find the injected typo: `Issue: Potential typo 'Nrth'`, `Suggestion: 'North'`.
    - It shows the confidence level and affected row count.
    - *Action*: Click the checkbox to select the fix, then click **Apply Selected Fixes**.
    - *Talking Point*: "The system uses fuzzy matching logic to identify inconsistencies. It doesn't just flag them; it generates the SQL `UPDATE` statement to fix them with one click."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~371)
        - Logic: `ml_engine.py` (Lines ~397-430, `suggest_data_repairs`)

================================================================================
SCENARIO 9: AI AUTO-DASHBOARDS (Instant Viz)
================================================================================
*Goal: Zero-effort visualization.*

1.  **Navigate to "AI Auto-Dashboards"**.
2.  **Setup**: Select `fact_retail_sales`.
3.  **The Reveal**:
    - The system automatically detects the Date column and the Sales column.
    - It renders a Time Series chart without you asking for it.
    - *Talking Point*: "I didn't configure a chart type. The AI analyzed the schema, saw a date and a float, and decided a Time Series was the best way to visualize this data."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~468)
        - Logic: `ml_engine.py` (Lines ~259-304, `recommend_visualization`)

================================================================================
SCENARIO 10: WHAT-IF SIMULATOR
================================================================================
*Goal: Strategic planning.*

1.  **Navigate to "What-If Simulator"**.
2.  **Action**:
    - Move the "Marketing Spend Increase" slider to +20%.
    - Move "Pricing Adjustment" to -5%.
3.  **The Reveal**:
    - The "Projected Revenue" number updates instantly.
    - The chart shows the "Simulated Scenario" diverging from the "Baseline".
    - *Talking Point*: "This allows business stakeholders to play with variables and see the potential financial impact in real-time."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~440-450)
        - Logic: Inline simulation logic in `analytics_ui.py`.

================================================================================
SCENARIO 11: PREDICTIVE INSIGHTS (Forecasting)
================================================================================
*Goal: Looking forward.*

1.  **Navigate to "Predictive Insights"**.
2.  **Setup**: Select `fact_retail_sales`.
3.  **The Reveal**:
    - Show the historical line (solid).
    - Show the forecast line (dashed) extending into the future.
    - Point out any red dots (Anomalies).
4.  **Action**: Select a date in the "Anomaly Feedback" dropdown and click **Dismiss**.
    - *Talking Point*: "This is Human-in-the-Loop AI. By dismissing this, I'm retraining the model to ignore similar patterns in the future, reducing alert fatigue."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~180-215)
        - Logic: `ml_engine.py` (Lines ~40-57 `detect_anomalies`, Lines ~134-165 `generate_forecast`)

================================================================================
SCENARIO 12: DATA GOVERNANCE (Quality Gates)
================================================================================
*Goal: Prove that the AI is trained on trusted data.*

1.  **Navigate to "Data Explorer"**.
2.  **Context**: "How do we know the data feeding these AI models is accurate? We have automated governance gates."
3.  **Action**:
    - Select Table: `data_quality_rules`.
    - Show the rules: e.g., `Sales_Not_Negative` (Severity: FAIL).
    - *Talking Point*: "We define rules as metadata. If 'Sales' is negative, the pipeline halts immediately."
4.  **Action**:
    - Select Table: `data_quality_run_logs`.
    - Sort by `check_timestamp` DESC.
    - Point to the row with `status = FAIL` and `failing_row_count = 12`.
    - *Talking Point*: "Look hereâ€”5 days ago, a bad file was uploaded with negative sales numbers. The system caught it, blocked the load, and alerted the team. No bad data reached the dashboard."
    - *Code Reference*:
        - Setup: `setup_demo_environment.py` (Lines ~180-210)
        - Logic: `02_setup_data_governance.sql` (Stored Procedure `sp_execute_data_quality_checks`)

================================================================================
SCENARIO 13: AUTOMATED ENRICHMENT
================================================================================
*Goal: Show how the system adds value to raw data automatically.*

1.  **Navigate to "Data Explorer"**.
2.  **Context**: "Raw data is often incomplete. For example, transactions might have a Product ID but missing the Category."
3.  **Action**:
    - Select Table: `data_enrichment_rules`.
    - Show the rule: `Enrich_Product_Category`.
    - *Talking Point*: "We don't write Python code to fix this. We just add a rule: 'If Category is missing, look it up in the Product Master table using the SKU'. The pipeline handles the join automatically."
    - *Code Reference*:
        - Setup: `setup_demo_environment.py` (Lines ~197-202)
        - Logic: `README.md` (Section: Automated Data Enrichment)

================================================================================
END OF DEMO
================================================================================

================================================================================
APPENDIX: DEEP DIVE TECHNICAL REFERENCE & LEARNING GUIDE
================================================================================
This section provides a technical deep-dive into the AI and Analytics concepts 
implemented in this framework. Use this to understand the "How" and "Why" behind 
the features.

Each section includes clear ASCII visualizations and simplified explanations 
to make abstract concepts easy to understand.

--------------------------------------------------------------------------------
1. LARGE LANGUAGE MODELS (LLMs) & GENERATIVE AI
--------------------------------------------------------------------------------
*   **Concept**: Imagine a super-advanced auto-complete on your phone. It doesn't 
    just guess the next word based on the last word; it looks at the entire book 
    you've written so far to guess the next word perfectly.

*   **Key Components Simplified**:
    1.  **Tokenization (The Word Chopper)**: Computers can't read words. They only 
        read numbers. This step chops words into number chunks.
    2.  **Embeddings (The Map of Meaning)**: Every word is turned into a list of 
        coordinates (like GPS). Words with similar meanings (Dog, Puppy) are close 
        together on the map.
    3.  **Self-Attention (The Spotlight)**: When reading a sentence, the model 
        shines a "spotlight" on other relevant words to understand context.
        *   *Example*: In "The bank of the river", when the model reads "bank", 
            it shines a spotlight on "river" to know it's not a money bank.
    4.  **Feed-Forward Networks (The Filing Cabinet)**: After understanding the 
        context, the model looks up facts and patterns in its internal memory 
        (its "filing cabinet") to process the information.

    **VISUALIZATION 1.1: TOKENIZATION (Text to Numbers)**
    *Caption: The computer converts language into a format it can process.*

    [Input Text]: "The cat sat"
          |
          v
    +-----------------------------------+
    | TOKENIZER (The Chopper)           |
    | "The" -> 101                      |
    | "cat" -> 754                      |
    | "sat" -> 882                      |
    +-----------------------------------+
          |
          v
    [Output IDs]: [101, 754, 882]

    **VISUALIZATION 1.2: EMBEDDINGS (Numbers to Meaning)**
    *Caption: Mapping words to a 3D space (simplified).*

    [ID: 754 ("cat")]
          |
          v
    +-----------------------------------+
    | EMBEDDING LAYER                   |
    | Coordinates: [0.9, 0.1, -0.5]     | <--- "Furry", "Small", "Pet"
    +-----------------------------------+

    **VISUALIZATION 1.3: SELF-ATTENTION (The Spotlight)**
    *Caption: How the model connects related words.*

    Sentence: "The animal didn't cross the street because it was too tired."

    Focus Word: [it]
          |
          +-----> Looks at "animal" (Strong Connection: 90%)
          |
          +-----> Looks at "street" (Weak Connection: 5%)
          |
          +-----> Looks at "tired"  (Medium Connection: 40%)

    *Result*: The model knows "it" refers to the "animal", not the "street".

    **VISUALIZATION 1.4: FEED-FORWARD (The Processing)**
    *Caption: Thinking about the information.*

    [Contextualized Word Info]
          |
          v
    +-----------------------------------+
    | FEED-FORWARD NETWORK              |
    | (Internal Knowledge Bank)         |
    | 1. Check grammar rules.           |
    | 2. Check facts about animals.     |
    | 3. Prepare next prediction.       |
    +-----------------------------------+
          |
          v
    [Processed Info]

    **VISUALIZATION 1.5: PROBABILITIES (The Dice Roll)**
    *Caption: Predicting the next word.*

    Context: "The cat sat on the..."
          |
          v
    +-----------------------+
    | PREDICTION SCORES     |
    | "mat"   : 80%         | <--- Most Likely
    | "floor" : 15%         |
    | "moon"  : 0.01%       |
    +-----------------------+

    **VISUALIZATION 1.6: TEMPERATURE (The Randomness Dial)**
    *Caption: Controlling creativity.*

    [Scores]: Mat(80%), Floor(15%)

    Temp = 0.1 (Low/Focused)      Temp = 0.9 (High/Creative)
    +----------------------+      +----------------------+
    | Picks "Mat" (100%)   |      | Picks "Floor" (Chance)|
    |                      |      |                      |
    | Result: Predictable  |      | Result: Surprising   |
    | Use for: SQL, Code   |      | Use for: Stories     |
    +----------------------+      +----------------------+

    **VISUALIZATION 1.7: CONTEXT WINDOW (Short-Term Memory)**
    *Caption: The sliding window of memory.*

    [ Memory Capacity: 4 Blocks ]

    Time 1: [Hi] [My] [Name] [Is]
    Time 2: [My] [Name] [Is] [Bob]  <--- "Hi" fell off the edge!

    *Result*: If the conversation is too long, the AI forgets the beginning.

    **VISUALIZATION 1.8: HALLUCINATION (The Guess)**
    *Caption: When the AI doesn't know, it guesses confidently.*

    Question: "Who is the CEO of FakeCorp?"
          |
    [Database Check]: Empty (No knowledge)
          |
    [Pattern Match]: "CEOs are often named John."
          |
    [Output]: "John Smith is the CEO." (False!)

    **VISUALIZATION 1.9: PROMPT ENGINEERING (The Instructions)**
    *Caption: How we talk to the model.*

    +--------------------------------------------------+
    | SYSTEM: "You are a helpful SQL expert."          | <--- Persona
    +--------------------------------------------------+
    | USER:   "Show me sales."                         | <--- Task
    +--------------------------------------------------+
    | CONTEXT: "Table: Sales(Date, Amount)"            | <--- Knowledge
    +--------------------------------------------------+
    | OUTPUT:  "SELECT * FROM Sales"                   |
    +--------------------------------------------------+

--------------------------------------------------------------------------------
2. TEXT-TO-SQL
--------------------------------------------------------------------------------
*   **Concept**: Translating human language ("Show me sales") into database 
    language ("SELECT * FROM...").

    **VISUALIZATION 2.1: THE PIPELINE**
    *Caption: From question to answer.*

    [User Question] --> [Prompt Builder] --> [LLM] --> [SQL Code] --> [Database]

    **VISUALIZATION 2.2: SCHEMA INJECTION (The Map)**
    *Caption: Giving the AI a map of the database without giving it the data.*

    Database:
    [Table: Users | Columns: ID, Name, Email]

    Prompt to AI:
    "I have a table named 'Users' with columns 'ID', 'Name', 'Email'.
     Write a query to count them."

    *Note*: We did NOT send the actual names or emails. Just the structure.

    **VISUALIZATION 2.3: THE VALIDATOR (The Security Guard)**
    *Caption: Stopping dangerous commands.*

    [Generated SQL]
          |
          v
    +---------------------------+
    | SECURITY CHECK            |
    | Contains 'DROP'?   [NO]   |
    | Contains 'DELETE'? [NO]   |
    | Contains 'SELECT'? [YES]  |
    +---------------------------+
          |
          v
    [Execute Query]

    **VISUALIZATION 2.4: AMBIGUITY RESOLUTION**
    *Caption: Guessing intent.*

    User: "Sales by year"
    DB:   Has [OrderDate] and [ShipDate].

    AI Thought Process:
    "Usually 'Sales' implies when the order happened, not when it shipped."
    -> Selects [OrderDate].

    **VISUALIZATION 2.5: SELF-CORRECTION**
    *Caption: Trying again when things fail.*

    Attempt 1: "SELECT date FROM sales"
    Error:     "Column 'date' does not exist."
          |
          v
    [Feedback Loop to AI]
    "You made a mistake. The column is named 'OrderDate'."
          |
          v
    Attempt 2: "SELECT OrderDate FROM sales" (Success!)

--------------------------------------------------------------------------------
3. AGENTIC AI (Autonomous Agents)
--------------------------------------------------------------------------------
*   **Concept**: An AI that can use tools (like a calculator, database, or search 
    engine) to solve multi-step problems.

    **VISUALIZATION 3.1: THE ReAct LOOP**
    *Caption: Reason -> Act -> Observe.*

    +-------------------------------------------------------+
    | GOAL: "Find the error and fix it."                    |
    +-------------------------------------------------------+
              |
    +---------v---------+      +------------------------+
    | 1. THOUGHT        |      | 2. ACTION              |
    | "I need to see    |----->| Call Tool:             |
    |  the logs first." |      | read_logs()            |
    +-------------------+      +-----------+------------+
                                           |
    +-------------------+      +-----------v------------+
    | 4. THOUGHT        |      | 3. OBSERVATION         |
    | "I see an error.  |<-----| "Error: Null Value     |
    |  I will fix it."  |      |  at row 5."            |
    +-------------------+      +------------------------+

    **VISUALIZATION 3.2: TOOL SELECTION (The Toolbox)**
    *Caption: Choosing the right tool for the job.*

    User: "What is 50 * 23?"  --> [Calculator Tool]
    User: "Who is the CEO?"   --> [Search Tool]
    User: "Show me sales."    --> [Database Tool]

    **VISUALIZATION 3.3: PLANNING (The Checklist)**
    *Caption: Breaking a big goal into small steps.*

    Goal: "Write a report on sales."
    1. [ ] Query database for sales numbers.
    2. [ ] Calculate growth percentage.
    3. [ ] Summarize findings in text.

--------------------------------------------------------------------------------
4. RAG (RETRIEVAL-AUGMENTED GENERATION) & VECTOR SEARCH
--------------------------------------------------------------------------------
*   **Concept**: Giving the AI an "Open Book Exam". Instead of memorizing everything, 
    it looks up the answer in your documents before answering.

    **VISUALIZATION 4.1: RAG ARCHITECTURE**
    *Caption: The flow of information.*

    [User Question]
          |
          v
    [Search Database] --> [Find Relevant Documents]
          |
          v
    [Combine Question + Documents]
          |
          v
    [Send to LLM] --> [Answer based on Documents]

    **VISUALIZATION 4.2: VECTOR SPACE (The Map of Meaning)**
    *Caption: How computers know "Dog" is close to "Puppy".*

       |
       |       (King)
       |          ^
       |          |
       |       (Man) ----> (Woman)
       |
       |                   (Queen)
       +----------------------------

    *Math*: King - Man + Woman = Queen.

    **VISUALIZATION 4.3: SEMANTIC VS KEYWORD**
    *Caption: Understanding meaning vs matching letters.*

    Query: "My screen is cracked."

    Keyword Search:
    - Looks for word "cracked".
    - Misses: "Display shattered".

    Semantic Search (Vectors):
    - Looks for concept "Broken Screen".
    - Finds: "Display shattered" (Because they are close on the map).

--------------------------------------------------------------------------------
5. MULTI-MODAL AI
--------------------------------------------------------------------------------
*   **Concept**: AI that can see, hear, and read.

    **VISUALIZATION 5.1: IMAGE PATCHING**
    *Caption: Breaking an image into words.*

    [ IMAGE ]  -->  [ 1 ] [ 2 ] [ 3 ]
                    [ 4 ] [ 5 ] [ 6 ]
                    [ 7 ] [ 8 ] [ 9 ]

    The AI treats each square (patch) like a word in a sentence.

    **VISUALIZATION 5.2: CROSS-ATTENTION**
    *Caption: Connecting text to image.*

    Text: "Find the dog."
    Image: [Tree] [Grass] [Dog] [Sky]

    Attention:
    "Find" --> [Dog] (High Match)
    "Find" --> [Tree] (Low Match)

    **VISUALIZATION 5.3: SPATIAL AWARENESS**
    *Caption: Knowing where things are.*

    Invoice:
    +------------------+
    | Total:           |
    |           $500   |
    +------------------+

    Traditional OCR: Reads "Total: $500" (Maybe).
    Vision AI: Sees "$500" is to the right of "Total:" and links them.

--------------------------------------------------------------------------------
6. PRESCRIPTIVE ANALYTICS & OPTIMIZATION
--------------------------------------------------------------------------------
*   **Concept**: The GPS for business. It doesn't just tell you where you are 
    (Descriptive) or where you are going (Predictive), it tells you which turn 
    to take (Prescriptive).

    **VISUALIZATION 6.1: THE OPTIMIZATION LOOP**
    *Caption: Climbing the hill to the highest profit.*

          (Profit Peak)
               / \
              /   \
             /     \
    (Start) *       \
             \       \

    Step 1: Check slope. Go up? Yes.
    Step 2: Check slope. Go up? Yes.
    Step 3: Slope is flat. Stop. You are at the top.

    **VISUALIZATION 6.2: DIGITAL TWIN**
    *Caption: A simulator of your business.*

    [Inputs: Price, Marketing] --> [MATH MODEL] --> [Output: Predicted Sales]

    We use this simulator to test 1,000 combinations in a second.

    **VISUALIZATION 6.3: SENSITIVITY**
    *Caption: How fragile is the plan?*

    Plan A: Profit $100. (If costs rise 1%, Profit drops to $0). -> RISKY.
    Plan B: Profit $90.  (If costs rise 1%, Profit drops to $89). -> SAFE.

--------------------------------------------------------------------------------
7. ANOMALY DETECTION (Unsupervised Learning)
--------------------------------------------------------------------------------
*   **Concept**: Finding the needle in the haystack.

    **VISUALIZATION 7.1: ISOLATION FOREST**
    *Caption: It's easier to cut a lone piece of cake than a piece in the middle.*

    Data:  [ . . . . . . ]           [ . ] <--- Anomaly

    Cut 1: [ . . . . . . ] |         [ . ]

    Result: The anomaly is isolated in 1 cut.
            The normal points need many cuts to separate.
            Short cuts = Anomaly.

    **VISUALIZATION 7.2: TIME SERIES DECOMPOSITION**
    *Caption: Taking apart the signal.*

    [Total Sales]
         =
    [Trend (Going up)]
         +
    [Seasonality (High on weekends)]
         +
    [Noise (Random bumps)]

    If the Noise is huge, it's an anomaly.

--------------------------------------------------------------------------------
8. DATA GOVERNANCE & QUALITY
--------------------------------------------------------------------------------
*   **Concept**: The bouncer at the club. If your ID (data) isn't valid, you don't 
    get in.

    **VISUALIZATION 8.1: THE QUALITY GATE**
    *Caption: Stopping bad data.*

    [Data Stream] --> [Check: Sales > 0?] --> [Check: Valid Date?] --> [Database]
                             |                        |
                             v                        v
                           [Fail]                   [Fail]
                             |
                             v
                       [Reject File]

    **VISUALIZATION 8.2: SCHEMA DRIFT**
    *Caption: Handling changes.*

    Expected: [Name, Age]
    Received: [Name, Age, Phone]

    Action: Alert! Structure changed.

--------------------------------------------------------------------------------
9. AUTOMATED ENRICHMENT
--------------------------------------------------------------------------------
*   **Concept**: Auto-complete for data.

    **VISUALIZATION 9.1: THE JOIN**
    *Caption: Filling in the blanks.*

    Table A (Sales): [Item: "iPhone", Category: ???]
    Table B (Master):[Item: "iPhone", Category: "Phone"]

    Result:          [Item: "iPhone", Category: "Phone"]

    **VISUALIZATION 9.2: FUZZY MATCH**
    *Caption: Handling typos.*

    Input: "Aplle"
    Master List: ["Apple", "Banana", "Pear"]

    Compare "Aplle" vs "Apple" -> 90% Match.
    Action: Auto-correct to "Apple".

--------------------------------------------------------------------------------
10. THE FULL MODERN DATA STACK
--------------------------------------------------------------------------------
*   **Concept**: How it all fits together.

    **VISUALIZATION 10.1: FULL STACK**
    *Caption: From raw file to dashboard.*

    [Raw File]
        |
        v
    [Ingestion Engine (Dagster)]
        |
        v
    [Data Warehouse (SQL Server)] <---> [AI Engine (Python)]
        |
        v
    [Dashboard (Streamlit)]
