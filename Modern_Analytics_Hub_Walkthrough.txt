================================================================================
          MODERN ANALYTICS & AI HUB: THE ULTIMATE DEMO GUIDE
================================================================================

This is a comprehensive, step-by-step script for demonstrating the full power of 
the Modern Analytics & AI Hub. It is designed to "show off" the system's 
capabilities, moving from descriptive analytics to advanced AI and self-healing 
data pipelines.

--------------------------------------------------------------------------------
PHASE 1: ENVIRONMENT SETUP
--------------------------------------------------------------------------------

1.  **Install Dependencies & Drivers**
    Open your terminal in the project root and run:
    > pip install pandas sqlalchemy pyodbc openai plotly streamlit scipy scikit-learn faker
    *Ensure you have the ODBC Driver 17 for SQL Server installed.*

2.  **Configure Environment Variables**
    Create or edit the `.env` file in the project root:
    - DB_SERVER=...
    - DB_DATABASE=...
    - DB_USERNAME=...
    - DB_PASSWORD=...
    - OPENAI_API_KEY=sk-... (Crucial for Text-to-SQL, Semantic Search, and Vision)

3.  **Generate the "Perfect" Demo Data**
    Run the setup script. This doesn't just create tables; it injects specific
    statistical patterns, anomalies, and text correlations that the AI is 
    programmed to find later.
    > python scripts/setup_demo_environment.py
    
    *Check Output*: Look for "ðŸŽ‰ Demo Environment Setup Complete!".

--------------------------------------------------------------------------------
PHASE 2: GENERATE AI INSIGHTS (BATCH LAYER)
--------------------------------------------------------------------------------

Before the demo, we need the "Batch AI" to process history, train models, and 
detect the anomalies we injected.

1.  Start Dagster:
    > dagster dev

2.  Open the Dagster UI (http://localhost:3000).
3.  Locate the asset group `analytics_ai` (or search for `run_predictive_analytics`).
4.  Click **Materialize** on the `run_predictive_analytics` asset.
    *   *Behind the Scenes*: The system pulls the last year of data, trains an 
        Isolation Forest model to find outliers, runs a Prophet forecast for the 
        next 30 days, and saves all predictions to the SQL database.

--------------------------------------------------------------------------------
PHASE 3: THE "SHOW OFF" DEMO SCRIPT (UI LAYER)
--------------------------------------------------------------------------------

Launch the Analytics UI in a new terminal:
> streamlit run analytics_ui.py

**Login**: Username: `admin` / Password: `admin123`

================================================================================
SCENARIO 1: THE EXECUTIVE OVERVIEW (Generative AI / LLM)
================================================================================
*Goal: Show that the system understands data context without human input.*

1.  **Navigate to "Dashboard"**.
2.  **Point out the Metrics**:
    - "AI Detected Anomalies": Point out the red number (e.g., 5).
    - "Failed Runs": Point out the system health.
3.  **The "Wow" Factor**: Scroll to **Automated Data Story**.
    - *Action*: Read the first bullet point aloud.
    - *Talking Point*: "I didn't write this report. A Generative AI (LLM) agent analyzed the raw pipeline logs and anomaly tables to generate this executive summary in real-time."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~112-117)
        - Logic: `ml_engine.py` (Lines ~198-226, `generate_data_story`)

================================================================================
SCENARIO 2: CONVERSATIONAL ANALYTICS (LLM-Powered Text-to-SQL)
================================================================================
*Goal: Show democratization of data access.*

1.  **Navigate to "Conversational Analytics"**.
2.  **The Query**: Type: `Show me total sales by region for last month`
3.  **The Result**:
    - Show the generated SQL code block (proving transparency/security).
    - Show the resulting data table.
    - *Talking Point*: "This uses an LLM to translate natural language into SQL. It understands 'last month' and 'sales', writes valid SQL, checks it for safety, and executes it."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~153-167)
        - Logic: `ml_engine.py` (Lines ~238-257, `generate_sql_from_question`)

================================================================================
SCENARIO 3: AGENTIC ANALYST (Autonomous AI Agent)
================================================================================
*Goal: Show the AI planning and executing a complex task autonomously.*

1.  **Navigate to "Agentic Analyst"**.
2.  **The Goal**: Enter a high-level goal:
    `Analyze the sales trend for the last 3 months, identify any significant drops, and check customer feedback for those periods to explain why.`
3.  **Action**: Click **Launch Agent**.
4.  **The Reveal**:
    - **Planning (Chain-of-Thought)**: Show the "Agent is planning..." expander. It breaks the goal into steps (Data Retrieval -> Analysis).
        - *Learning Point*: This simulates "Chain of Thought" reasoning, where the LLM breaks a complex problem into sub-tasks before acting.
    - **Execution (Tool Use)**:
        - **Tool 1 (SQL Generator)**: It generates SQL to get sales data.
        - **Tool 2 (Data Analyzer)**: It calculates metrics from the retrieved dataframe.
        - **Tool 3 (Storyteller)**: It generates a final report explaining the drop (referencing the "server error" feedback injected in the demo data).
    - *Talking Point*: "This isn't just a chatbot. It's an agent. It formulated a plan, queried the database, found the anomaly, and then correlated it with customer feedback to give me a complete answer."
    - *Code Reference*:
        - UI Implementation: `analytics_ui.py` (Lines ~185-246)
            - *Planning Simulation*: Lines ~201-207
            - *Context Retrieval*: Lines ~212-214
            - *Tool Execution*: Lines ~217-240
        - AI Logic: `ml_engine.py`
            - *Text-to-SQL Tool*: Lines ~238-257 (`generate_sql_from_question`)
            - *Insight Generator Tool*: Lines ~198-226 (`generate_data_story`)

================================================================================
SCENARIO 4: DIAGNOSTIC AI (Root Cause Analysis)
================================================================================
*Goal: Solve a business mystery in seconds.*

1.  **Navigate to "Root Cause Analysis"**.
2.  **Setup**:
    - Table: `fact_retail_sales`
    - Metric: `SalesAmount`
    - Target Date: Select the date **3 days ago** (The setup script injected a 80% drop here).
3.  **Action**: Click **Analyze Drivers**.
4.  **The Reveal**:
    - The system will display `Region = North` and `Category = Electronics` with a large negative impact (e.g., -80%).
    - *Talking Point*: "Usually, an analyst would spend hours slicing data in Excel to find this. The AI scanned every combination of dimensions and found the exact source of the crash: Electronics in the North region."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~261)
        - Logic: `ml_engine.py` (Lines ~60-115, `analyze_root_cause`)

================================================================================
SCENARIO 5: SEMANTIC INTELLIGENCE (LLM RAG / Vector Search)
================================================================================
*Goal: Connect the quantitative drop to qualitative feedback.*

1.  **Navigate to "Semantic Search"**.
2.  **Context**: "We know sales dropped in North/Electronics. Was it a pricing issue? A product issue?"
3.  **Setup**:
    - Table: `fact_retail_sales`
    - Column: `CustomerFeedback`
    - Query: `technical issues during checkout`
4.  **Action**: Click **Search**.
5.  **The Reveal**:
    - The results show rows containing: *"Cannot checkout, server error"*.
    - *Crucial Point*: Note that the word "technical" might not appear in the result.
    - *Talking Point*: "This uses LLM Embeddings for Semantic Search. I searched for 'technical issues', and it found 'server error' because the LLM understands they mean the same thing. Traditional keyword search would have failed here."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~351)
        - Logic: `ml_engine.py` (Lines ~359-395, `perform_semantic_search`)

================================================================================
SCENARIO 6: PRESCRIPTIVE ANALYTICS (Optimization)
================================================================================
*Goal: Move from "What happened?" to "What should we do?".*

1.  **Navigate to "Prescriptive Optimization"**.
2.  **Context**: "We need to recover that lost revenue next week. How much should we spend on ads?"
3.  **Setup**:
    - Table: `fact_retail_sales`
    - Target: `SalesAmount`
    - Inputs: Select `MarketingSpend` and `DiscountRate`.
4.  **Action**: Click **Run Optimization**.
5.  **The Reveal**:
    - The system displays a "Projected Maximum Target".
    - It gives specific recommendations: e.g., *MarketingSpend: $840.50*, *DiscountRate: 0.15*.
    - *Talking Point*: "The system modeled the elasticity of demand based on our history and used a mathematical solver to give us the optimal marketing mix."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~320)
        - Logic: `ml_engine.py` (Lines ~306-357, `optimize_business_objective`)

================================================================================
SCENARIO 7: MULTI-MODAL ANALYSIS (Vision LLM)
================================================================================
*Goal: Show the system handling unstructured, non-text data.*

1.  **Navigate to "Multi-Modal Analysis"**.
2.  **Context**: "We just received a vendor invoice as a scanned image."
3.  **Action**:
    - Upload a sample image (e.g., a screenshot of an invoice or a receipt).
    - In "Fields to Extract", type: `Invoice Number, Total Amount, Vendor Name, Date`.
    - Click **Extract Data**.
4.  **The Reveal**:
    - The JSON result appears with the correct data extracted from the pixels.
    - *Talking Point*: "We are using a Multi-Modal LLM (GPT-4 Vision) to turn pixels into structured database records instantly."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~406)
        - Logic: `ml_engine.py` (Lines ~432-483, `extract_structured_data`)

================================================================================
SCENARIO 8: AUTONOMOUS DATA REPAIR (Self-Healing)
================================================================================
*Goal: Show AI maintaining data quality.*

1.  **Navigate to "Autonomous Data Repair"**.
2.  **Context**: "Data entry errors happen. Let's see if the AI can find them."
3.  **Setup**:
    - Table: `fact_retail_sales`
4.  **Action**: Click **Scan for Issues**.
5.  **The Reveal**:
    - The system should find the injected typo: `Issue: Potential typo 'Nrth'`, `Suggestion: 'North'`.
    - It shows the confidence level and affected row count.
    - *Action*: Click the checkbox to select the fix, then click **Apply Selected Fixes**.
    - *Talking Point*: "The system uses fuzzy matching logic to identify inconsistencies. It doesn't just flag them; it generates the SQL `UPDATE` statement to fix them with one click."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~371)
        - Logic: `ml_engine.py` (Lines ~397-430, `suggest_data_repairs`)

================================================================================
SCENARIO 9: AI AUTO-DASHBOARDS (Instant Viz)
================================================================================
*Goal: Zero-effort visualization.*

1.  **Navigate to "AI Auto-Dashboards"**.
2.  **Setup**: Select `fact_retail_sales`.
3.  **The Reveal**:
    - The system automatically detects the Date column and the Sales column.
    - It renders a Time Series chart without you asking for it.
    - *Talking Point*: "I didn't configure a chart type. The AI analyzed the schema, saw a date and a float, and decided a Time Series was the best way to visualize this data."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~468)
        - Logic: `ml_engine.py` (Lines ~259-304, `recommend_visualization`)

================================================================================
SCENARIO 10: WHAT-IF SIMULATOR
================================================================================
*Goal: Strategic planning.*

1.  **Navigate to "What-If Simulator"**.
2.  **Action**:
    - Move the "Marketing Spend Increase" slider to +20%.
    - Move "Pricing Adjustment" to -5%.
3.  **The Reveal**:
    - The "Projected Revenue" number updates instantly.
    - The chart shows the "Simulated Scenario" diverging from the "Baseline".
    - *Talking Point*: "This allows business stakeholders to play with variables and see the potential financial impact in real-time."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~440-450)
        - Logic: Inline simulation logic in `analytics_ui.py`.

================================================================================
SCENARIO 11: PREDICTIVE INSIGHTS (Forecasting)
================================================================================
*Goal: Looking forward.*

1.  **Navigate to "Predictive Insights"**.
2.  **Setup**: Select `fact_retail_sales`.
3.  **The Reveal**:
    - Show the historical line (solid).
    - Show the forecast line (dashed) extending into the future.
    - Point out any red dots (Anomalies).
4.  **Action**: Select a date in the "Anomaly Feedback" dropdown and click **Dismiss**.
    - *Talking Point*: "This is Human-in-the-Loop AI. By dismissing this, I'm retraining the model to ignore similar patterns in the future, reducing alert fatigue."
    - *Code Reference*:
        - UI: `analytics_ui.py` (Lines ~180-215)
        - Logic: `ml_engine.py` (Lines ~40-57 `detect_anomalies`, Lines ~134-165 `generate_forecast`)

================================================================================
SCENARIO 12: DATA GOVERNANCE (Quality Gates)
================================================================================
*Goal: Prove that the AI is trained on trusted data.*

1.  **Navigate to "Data Explorer"**.
2.  **Context**: "How do we know the data feeding these AI models is accurate? We have automated governance gates."
3.  **Action**:
    - Select Table: `data_quality_rules`.
    - Show the rules: e.g., `Sales_Not_Negative` (Severity: FAIL).
    - *Talking Point*: "We define rules as metadata. If 'Sales' is negative, the pipeline halts immediately."
4.  **Action**:
    - Select Table: `data_quality_run_logs`.
    - Sort by `check_timestamp` DESC.
    - Point to the row with `status = FAIL` and `failing_row_count = 12`.
    - *Talking Point*: "Look hereâ€”5 days ago, a bad file was uploaded with negative sales numbers. The system caught it, blocked the load, and alerted the team. No bad data reached the dashboard."
    - *Code Reference*:
        - Setup: `setup_demo_environment.py` (Lines ~180-210)
        - Logic: `02_setup_data_governance.sql` (Stored Procedure `sp_execute_data_quality_checks`)

================================================================================
SCENARIO 13: AUTOMATED ENRICHMENT
================================================================================
*Goal: Show how the system adds value to raw data automatically.*

1.  **Navigate to "Data Explorer"**.
2.  **Context**: "Raw data is often incomplete. For example, transactions might have a Product ID but missing the Category."
3.  **Action**:
    - Select Table: `data_enrichment_rules`.
    - Show the rule: `Enrich_Product_Category`.
    - *Talking Point*: "We don't write Python code to fix this. We just add a rule: 'If Category is missing, look it up in the Product Master table using the SKU'. The pipeline handles the join automatically."
    - *Code Reference*:
        - Setup: `setup_demo_environment.py` (Lines ~197-202)
        - Logic: `README.md` (Section: Automated Data Enrichment)

================================================================================
END OF DEMO
================================================================================

================================================================================
APPENDIX: DEEP DIVE TECHNICAL REFERENCE & LEARNING GUIDE
================================================================================
This section provides a technical deep-dive into the AI and Analytics concepts 
implemented in this framework. Use this to understand the "How" and "Why" behind 
the features.

--------------------------------------------------------------------------------
1. LARGE LANGUAGE MODELS (LLMs) & GENERATIVE AI
--------------------------------------------------------------------------------
*   **Technical Definition**: LLMs are probabilistic models based on the **Transformer** 
    architecture (specifically the Decoder-only stack in GPT models). They are trained 
    on vast corpora to predict the next token (sub-word unit) in a sequence.
*   **Mechanism**:
    1.  **Tokenization**: Input text is converted into numerical tokens.
    2.  **Embeddings**: Tokens are mapped to vectors in a high-dimensional space.
    3.  **Self-Attention**: The model weighs the importance of different words in the 
        context window relative to each other (e.g., linking "bank" to "river" vs "money").
    4.  **Probabilistic Output**: The model outputs a probability distribution (logits) 
        over the vocabulary for the next token.
*   **Key Parameters**:
    *   **Temperature**: Controls randomness. Low (0.1) makes the model deterministic 
        and focused (good for SQL/Code). High (0.8) makes it creative (good for stories).
    *   **Context Window**: The maximum number of tokens the model can process at once 
        (Input + Output).
*   **In this Demo**: Used in `generate_data_story` with a low temperature for factual 
    summarization of logs.
*   **Visual Concept**:
    ```
    [Input: "The cat sat"]
           |
    (Tokenization: [102, 554, 998])
           |
    +---------------------+
    |  Transformer Layers | <--- Trained on Internet Data
    | (Self-Attention)    |      (Learns patterns/grammar)
    +---------------------+
           |
    (Probabilities: "on": 80%, "in": 10%)
           |
    [Output: "on"]
    ```

--------------------------------------------------------------------------------
2. TEXT-TO-SQL
--------------------------------------------------------------------------------
*   **Concept**: A specialized application of LLMs where the output is constrained 
    to valid SQL syntax based on a provided schema.
*   **Architecture**:
    *   **Input**: User Question + Database Schema (DDL or list of columns).
    *   **System Prompt**: Instructions to act as a SQL expert, use specific dialects 
        (T-SQL), and avoid hallucinations.
    *   **Output**: Executable SQL string.
*   **Security & Governance (Critical)**:
    *   **Schema Only**: We NEVER send actual row data to the LLM, only metadata 
        (Table names, Column names, Types). This preserves data privacy.
    *   **Validation**: The generated SQL is parsed (using Regex or AST) to block 
        destructive commands (`DROP`, `DELETE`, `TRUNCATE`) before execution.
*   **Implementation**: `MLEngine.generate_sql_from_question`. It constructs a 
    prompt containing `INFORMATION_SCHEMA` data and enforces a read-only policy.
*   **Visual Concept**:
    ```
    User: "Show sales"      Schema: [Table: Sales, Cols: Date, Amt]
           |                       |
           +-----------+-----------+
                       |
                 [ LLM Prompt ]
                       |
                 [ Generated SQL ]
           "SELECT sum(Amt) FROM Sales"
                       |
                (Security Check) -> [ Database Execution ] -> [ Result: $10k ]
    ```

--------------------------------------------------------------------------------
3. AGENTIC AI (Autonomous Agents)
--------------------------------------------------------------------------------
*   **Definition**: An Agent is an LLM loop equipped with **Tools** (functions) and 
    **Reasoning** capabilities (ReAct Pattern: Reason + Act).
*   **The ReAct Loop**:
    1.  **Thought**: The LLM analyzes the user goal and current state.
    2.  **Plan**: It determines the next step (e.g., "I need data").
    3.  **Action**: It selects a Tool (e.g., `run_sql_query`) and generates arguments.
    4.  **Observation**: The system executes the tool and feeds the output (DataFrame) 
        back to the LLM.
    5.  **Synthesis**: The LLM uses the observation to answer the user or plan the next step.
*   **Tools**: In our code, "Tools" are just Python functions wrapped in a way the 
    LLM can invoke (e.g., `generate_sql`, `generate_data_story`).
*   **Why it matters**: Standard LLMs are passive (Input -> Output). Agents are active; 
    they can perform multi-step workflows to solve open-ended problems.
*   **Visual Concept**:
    ```
    Goal: "Fix the data error"
           |
    +----->[ AGENT BRAIN (LLM) ]<-------+
    |      1. THOUGHT: "I need to find the error first."
    |      2. PLAN: "Run data scan tool."
    |      3. ACTION: Call Tool `scan_data()`
    |                                   |
    |      (Observation)                |
    +------ "Found typo in row 5" <-----+
           |
           4. THOUGHT: "Now I need to fix it."
           5. ACTION: Call Tool `fix_data(row=5)`
    ```

--------------------------------------------------------------------------------
4. RAG (RETRIEVAL-AUGMENTED GENERATION) & VECTOR SEARCH
--------------------------------------------------------------------------------
*   **Problem**: LLMs have a knowledge cutoff and don't know your private data.
*   **Solution (RAG)**: Retrieve relevant private data and inject it into the LLM's 
    context window at query time.
*   **Vector Embeddings**:
    *   We use an Embedding Model (e.g., `text-embedding-3-small`) to convert text 
        into **Dense Vectors** (arrays of floating-point numbers, e.g., 1536 dimensions).
    *   **Semantic Meaning**: In this vector space, concepts that are semantically 
        similar ("Dog" and "Puppy", or "Error 500" and "Crash") are located close 
        together geometrically.
*   **Cosine Similarity**: The mathematical operation used to measure the distance 
    between two vectors. A score of 1.0 means identical meaning; 0 means unrelated.
*   **Implementation**: `MLEngine.perform_semantic_search` calculates the cosine 
    similarity between the User Query vector and the Database Row vectors.
*   **Visual Concept**:
    ```
    User Query: "Server crash?"
           |
    [ Embedding Model ] ---> [ 0.1, 0.9, ... ] (Vector)
                                   |
                            [ Vector Database ]
                            (Compare vs stored docs)
                                   |
    +------------------------------+
    | Match: "Log: Error 500 detected"
    |
    +-> [ LLM Prompt: Context + Query ] -> "The logs show an Error 500..."
    ```

--------------------------------------------------------------------------------
5. MULTI-MODAL AI
--------------------------------------------------------------------------------
*   **Definition**: Models capable of processing and correlating information from 
    multiple modalities (Text, Images, Audio).
*   **Vision Transformers (ViT)**:
    *   Images are sliced into patches (e.g., 16x16 pixels).
    *   These patches are flattened and linearly projected into embeddings, similar 
        to word tokens.
    *   The Transformer processes these visual tokens alongside text tokens.
*   **Use Case**: OCR on steroids. Instead of just reading text characters, the model 
    understands **layout** and **context** (e.g., distinguishing a "Total" field 
    from a "Subtotal" based on position and font weight).
*   **In this Demo**: We use GPT-4o to extract JSON from invoice images.
*   **Visual Concept**:
    ```
    [ Image: Invoice ]      [ Text: "Extract Total" ]
           |                        |
    [ Patch 1 ][ Patch 2 ]          |
           |                        |
    [ Visual Encoder ]      [ Text Encoder ]
           |                        |
           +-----------+------------+
                       |
               [ Cross-Attention ] -> [ Output: "$500.00" ]
    ```

--------------------------------------------------------------------------------
6. PRESCRIPTIVE ANALYTICS & OPTIMIZATION
--------------------------------------------------------------------------------
*   **Hierarchy of Analytics**:
    1.  **Descriptive**: What happened? (Dashboards)
    2.  **Predictive**: What will happen? (Forecasting)
    3.  **Prescriptive**: How can we make it happen? (Optimization)
*   **Technical Implementation**:
    1.  **Digital Twin**: We train a regression model ($y = mx + b$) to approximate 
        the business process. This model predicts the Target (Sales) based on Inputs 
        (Marketing Spend, Price).
    2.  **Objective Function**: We define a mathematical function to maximize (Profit) 
        or minimize (Cost).
    3.  **Solver**: We use `scipy.optimize.minimize` (SLSQP algorithm) to mathematically 
        find the input values that yield the best output, subject to constraints 
        (e.g., Budget < $10k).
*   **Visual Concept**:
    ```
    [ Inputs (Variables) ]
    (Ad Spend, Price)
           |
           v
    [ Predictive Model ]  <--- (Learned from History)
    ( Sales = 2*Spend - 5*Price )
           |
           v
    [ Optimizer / Solver ] <--- Goal: Maximize Sales
           |                    Constraint: Spend < $1000
           v
    [ Recommendation ] -> "Set Spend = $999, Price = $10"
    ```

--------------------------------------------------------------------------------
7. ANOMALY DETECTION (Unsupervised Learning)
--------------------------------------------------------------------------------
*   **Algorithm**: **Isolation Forest**.
*   **Concept**: Anomalies are "few and different".
*   **How it works**:
    *   The algorithm builds an ensemble of random decision trees.
    *   At each node, it randomly selects a feature and a split value.
    *   **Normal points** are clustered together and require many splits to isolate.
    *   **Anomalies** are far apart and are isolated very quickly (short path length 
        from root to leaf).
*   **Scoring**: The average path length across all trees determines the anomaly score. 
    Shorter paths = Higher Anomaly Score.
*   **Contamination**: A hyperparameter defining the expected proportion of outliers 
    in the dataset (e.g., 0.05 or 5%).
*   **Visual Concept**:
    ```
       [ Root Node ]
       /           \
    [Split A]     [Split B]
     /     \       /     \
   (..)   (..)   (..)   [Anomaly]
   Normal Data          (Isolated quickly)
   (Deep in tree)       (Short path length)

   Path Length:
   Normal  = High (Hard to isolate)
   Anomaly = Low  (Easy to isolate)
    ```
