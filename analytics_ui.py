import streamlit as st
import pandas as pd
from sqlalchemy import create_engine, text
import os
import sys
from dotenv import load_dotenv
import plotly.express as px
import plotly.graph_objects as go
import time

# Add project root to path to import core modules
sys.path.append(os.path.dirname(__file__))
from elt_project.core.ml_engine import MLEngine

# Load environment variables
load_dotenv()

# Page Config
st.set_page_config(page_title="Analytics & AI Hub", page_icon="üìà", layout="wide")

# Database Connection
@st.cache_resource
def get_connection():
    db_connection_str = (
        f"mssql+pyodbc://{os.getenv('DB_USERNAME')}:{os.getenv('DB_PASSWORD')}@"
        f"{os.getenv('DB_SERVER')}/{os.getenv('DB_DATABASE')}?"
        f"driver={os.getenv('DB_DRIVER')}&TrustServerCertificate={os.getenv('DB_TRUST_SERVER_CERTIFICATE')}"
    )
    return create_engine(db_connection_str)

engine = get_connection()

# --- Caching Helper ---
@st.cache_data(ttl=600) # Cache data for 10 minutes
def run_query(query_str):
    with engine.connect() as conn:
        return pd.read_sql(query_str, conn)
 
# --- Authentication & RBAC System ---
# For seamless integration, authentication is bypassed. Defaulting to 'Admin' role.
if "user_role" not in st.session_state:
    st.session_state.user_role = "Admin"

# --- Sidebar ---
st.sidebar.title("Analytics & AI Hub")
st.sidebar.caption(f"üë§ Role: **{st.session_state.user_role}**")

# RBAC: Define accessible pages based on Role
# Since the role is defaulted to 'Admin', all pages are available.
available_pages = [
    "Dashboard", "Conversational Analytics", "Predictive Insights", "Root Cause Analysis", 
    "Clustering & Segmentation", "Prescriptive Optimization", "Semantic Search", 
    "Multi-Modal Analysis", "Autonomous Data Repair", "What-If Simulator", 
    "AI Auto-Dashboards", "Data Explorer", "Data Steward", "Data Observability", 
    "Configuration Manager"
]

page = st.sidebar.radio("Navigate", available_pages)

# --- Dashboard Page ---
if page == "Dashboard":
    st.title("üöÄ Executive Dashboard")
    st.markdown("Real-time overview of pipeline health and key metrics.")
    
    col1, col2, col3 = st.columns(3)
    
    # Use cached query function
    run_count = run_query("SELECT COUNT(*) as cnt FROM etl_pipeline_run_logs WHERE status = 'SUCCESS'").iloc[0]['cnt']
    fail_count = run_query("SELECT COUNT(*) as cnt FROM etl_pipeline_run_logs WHERE status = 'FAILURE'").iloc[0]['cnt']
    anomaly_count = run_query("SELECT COUNT(*) as cnt FROM analytics_predictions WHERE is_anomaly = 1").iloc[0]['cnt']

    col1.metric("Total Successful Runs", run_count)
    col2.metric("Failed Runs", fail_count, delta_color="inverse")
    col3.metric("AI Detected Anomalies", anomaly_count, delta_color="inverse")

    # Automated Data Storytelling
    st.subheader("üìù Automated Data Story")
    success_rate = (run_count / (run_count + fail_count)) * 100 if (run_count + fail_count) > 0 else 0
    
    metrics = {"success_rate": success_rate, "anomalies": anomaly_count, "total_runs": run_count + fail_count, "failed": fail_count}
    story = MLEngine.generate_data_story(metrics, context_str="Data Pipeline Automation System V3")
    st.info(story)

    if st.button("üìÑ Download Executive Report (HTML)"):
        st.download_button("Click to Save", data=f"<h1>Executive Report</h1><p>{story}</p><hr>Generated by Analytics Hub", file_name="report.html", mime="text/html")

    st.subheader("Recent Activity")
    history = run_query("SELECT TOP 10 pipeline_name, start_time, status FROM etl_pipeline_run_logs ORDER BY start_time DESC")
    st.dataframe(history, use_container_width=True)

# --- Conversational Analytics (NLQ) Page ---
elif page == "Conversational Analytics":
    st.title("üí¨ Conversational Analytics")
    st.markdown("Ask questions in plain English to generate insights, SQL, and visualizations.")

    if not os.getenv("OPENAI_API_KEY"):
        st.warning("‚ö†Ô∏è OpenAI API Key not found or library missing. Please set `OPENAI_API_KEY` in .env and install `openai`.")
        st.stop()

    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # React to user input
    if prompt := st.chat_input("Ex: Why did sales drop in Texas last week?"):
        # Display user message in chat message container
        st.chat_message("user").markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("assistant"):
            with st.spinner("Translating natural language to SQL..."):
                try:
                    # 1. Get Schema Context
                    schema_df = run_query("SELECT TABLE_NAME, COLUMN_NAME, DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME NOT LIKE 'sys%'")
                    schema_context = schema_df.to_csv(index=False)

                    sql_query = MLEngine.generate_sql_from_question(prompt, schema_context)
                    
                    # 3. Execute Query
                    result_df = run_query(sql_query)
                    
                    st.markdown(f"**Generated SQL:**")
                    st.code(sql_query, language="sql")
                    
                    st.markdown("**Result:**")
                    st.dataframe(result_df)
                    
                    st.session_state.messages.append({"role": "assistant", "content": f"Executed SQL: `{sql_query}`"})
                except Exception as e:
                    st.error(f"AI Error: {e}")
                    st.session_state.messages.append({"role": "assistant", "content": f"Error: {e}"})

# --- Predictive Insights Page ---
elif page == "Predictive Insights":
    st.title("ü§ñ AI & Predictive Analytics")
    
    tables = run_query("SELECT DISTINCT target_table FROM analytics_predictions")
    
    selected_table = st.selectbox("Select Data Source", tables['target_table'])
    
    if selected_table:
        query = f"""
            SELECT prediction_date, actual_value, predicted_value, is_anomaly 
            FROM analytics_predictions 
            WHERE target_table = '{selected_table}'
            ORDER BY prediction_date
        """
        data = run_query(query)
        
        # Visualization
        fig = go.Figure()
        
        # Actuals (if available in this table structure)
        if 'actual_value' in data.columns and data['actual_value'].notna().any():
            fig.add_trace(go.Scatter(x=data['prediction_date'], y=data['actual_value'], mode='lines', name='Actual Value'))
            
            # Anomalies
            anomalies = data[data['is_anomaly'] == 1]
            fig.add_trace(go.Scatter(x=anomalies['prediction_date'], y=anomalies['actual_value'], mode='markers', name='Anomaly', marker=dict(color='red', size=10)))

        # Forecasts
        forecasts = data[data['predicted_value'].notna()]
        if not forecasts.empty:
            fig.add_trace(go.Scatter(x=forecasts['prediction_date'], y=forecasts['predicted_value'], mode='lines', name='AI Forecast', line=dict(dash='dash')))

        st.plotly_chart(fig, use_container_width=True)
        
        # Human-in-the-Loop Feedback
        if not anomalies.empty:
            st.subheader("üõ†Ô∏è Anomaly Feedback")
            to_dismiss = st.selectbox("Select an anomaly to dismiss (False Positive)", anomalies['prediction_date'])
            if st.button("Dismiss Selected Anomaly"):
                with engine.begin() as conn:
                    conn.execute(text(f"UPDATE analytics_predictions SET is_anomaly = 0 WHERE target_table = '{selected_table}' AND prediction_date = '{to_dismiss}'"))
                st.success(f"Dismissed anomaly for {to_dismiss}. It will no longer appear in alerts.")
                st.rerun()

        st.info("üí° **AI Insight**: Anomalies detected in red. Dashed lines indicate future predictions generated by the ML Engine.")

# --- Root Cause Analysis Page ---
elif page == "Root Cause Analysis":
    st.title("üìâ Automated Root Cause Analysis")
    st.markdown("Automatically identify **why** a metric changed by scanning for drivers across all dimensions.")

    tables_df = run_query("SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'")
    selected_table = st.selectbox("Select Table", tables_df['TABLE_NAME'])

    if selected_table:
        # Get columns
        cols_df = run_query(f"SELECT TOP 0 * FROM {selected_table}")
        all_cols = cols_df.columns.tolist()
        numeric_cols = cols_df.select_dtypes(include=['float', 'int']).columns.tolist()
        date_cols = [c for c in all_cols if 'date' in c.lower() or 'time' in c.lower()]

        col1, col2, col3 = st.columns(3)
        date_col = col1.selectbox("Date Column", date_cols) if date_cols else None
        val_col = col2.selectbox("Metric to Analyze", numeric_cols) if numeric_cols else None
        
        if date_col and val_col:
            # Fetch aggregated data for selection
            ts_df = run_query(f"SELECT {date_col}, SUM({val_col}) as {val_col} FROM {selected_table} GROUP BY {date_col} ORDER BY {date_col}")
            
            # Plot trend
            fig = px.line(ts_df, x=date_col, y=val_col, title=f"Trend: {val_col}")
            st.plotly_chart(fig, use_container_width=True)

            # Select Dates
            dates = ts_df[date_col].dt.date.unique()
            target_date = col3.selectbox("Select Anomaly/Target Date", dates, index=len(dates)-1)
            
            if st.button("üîç Analyze Drivers"):
                # Fetch full data for the two relevant dates (Target vs Previous Day)
                compare_date = target_date - pd.Timedelta(days=1)
                full_data = run_query(f"SELECT * FROM {selected_table} WHERE CAST({date_col} AS DATE) IN ('{target_date}', '{compare_date}')")
                
                insights = MLEngine.analyze_root_cause(full_data, date_col, val_col, target_date, compare_date)
                
                st.subheader(f"Drivers of Change: {compare_date} vs {target_date}")
                if not insights:
                    st.warning("No significant drivers found or insufficient data.")
                else:
                    for item in insights:
                        delta_color = "inverse" if item['impact'] < 0 else "normal"
                        st.metric(
                            label=f"{item['dimension']} = {item['segment']}",
                            value=f"{item['curr_value']:,.0f}",
                            delta=f"{item['impact']:+,.0f} ({item['contribution_pct']:.1f}%)",
                            delta_color=delta_color
                        )

# --- Clustering & Segmentation Page ---
elif page == "Clustering & Segmentation":
    st.title("üß© Customer & Entity Segmentation")
    st.markdown("Use Unsupervised Learning (K-Means) to group similar data points automatically.")

    tables_df = run_query("SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'")
    selected_table = st.selectbox("Select Table", tables_df['TABLE_NAME'])

    if selected_table:
        # Fetch columns
        df_preview = run_query(f"SELECT TOP 5 * FROM {selected_table}")
        numeric_cols = df_preview.select_dtypes(include=['float', 'int']).columns.tolist()
        
        selected_features = st.multiselect("Select Features for Clustering", numeric_cols, default=numeric_cols, help="We've auto-selected all numeric columns. Remove any ID columns or irrelevant data.")
        n_clusters = st.slider("Number of Clusters", 2, 10, 3)

        if st.button("Run Clustering") and len(selected_features) >= 2:
            # Fetch full data for clustering
            full_df = run_query(f"SELECT {', '.join(selected_features)} FROM {selected_table}")
            
            with st.spinner("Running K-Means Algorithm..."):
                clustered_df = MLEngine.perform_clustering(full_df, selected_features, n_clusters)
            
            st.success("Clustering Complete!")
            
            # Visualize (Pairplot or Scatter)
            if len(selected_features) >= 2:
                fig = px.scatter(clustered_df, x=selected_features[0], y=selected_features[1], color='cluster_id', title=f"Segmentation: {selected_features[0]} vs {selected_features[1]}")
                st.plotly_chart(fig, use_container_width=True)

# --- Prescriptive Optimization Page ---
elif page == "Prescriptive Optimization":
    st.title("üéØ Prescriptive Analytics & Optimization")
    st.markdown("Don't just predict the future‚Äî**shape it**. Define a target to maximize, and the AI will recommend the optimal input values.")

    tables_df = run_query("SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'")
    selected_table = st.selectbox("Select Table", tables_df['TABLE_NAME'])

    if selected_table:
        df_preview = run_query(f"SELECT TOP 100 * FROM {selected_table}")
        numeric_cols = df_preview.select_dtypes(include=['float', 'int']).columns.tolist()

        col1, col2 = st.columns(2)
        target_col = col1.selectbox("Target to Maximize", numeric_cols, help="e.g., TotalSales, Profit")
        input_cols = col2.multiselect("Controllable Inputs", [c for c in numeric_cols if c != target_col], help="Variables you can change, e.g., Discount, AdSpend")

        if st.button("üöÄ Run Optimization") and target_col and input_cols:
            full_df = run_query(f"SELECT {target_col}, {', '.join(input_cols)} FROM {selected_table}")
            
            with st.spinner("Training regression model and solving optimization problem..."):
                result = MLEngine.optimize_business_objective(full_df, target_col, input_cols)
            
            if result.get("success"):
                st.success("Optimization Solved!")
                st.metric("Projected Maximum Target", f"{result['optimized_target']:,.2f}")
                
                st.subheader("Recommended Actions")
                rec_df = pd.DataFrame(list(result['recommendations'].items()), columns=["Input Variable", "Recommended Value"])
                st.dataframe(rec_df, use_container_width=True)
            else:
                st.error(f"Optimization failed: {result.get('message')}")

# --- Semantic Search (RAG) Page ---
elif page == "Semantic Search":
    st.title("üß† Semantic Search (RAG)")
    st.markdown("Search your data using **meaning**, not just keywords. Finds relevant rows even if exact words don't match.")

    tables_df = run_query("SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'")
    selected_table = st.selectbox("Select Table", tables_df['TABLE_NAME'])

    if selected_table:
        df_preview = run_query(f"SELECT TOP 5 * FROM {selected_table}")
        text_cols = df_preview.select_dtypes(include=['object']).columns.tolist()
        
        search_col = st.selectbox("Column to Search", text_cols)
        query = st.text_input("Search Query", placeholder="e.g., 'Customers complaining about late delivery'")

        if st.button("üîç Search") and query:
            # Fetch data (Limit for demo performance)
            full_df = run_query(f"SELECT TOP 200 * FROM {selected_table}")
            
            with st.spinner("Generating embeddings and calculating similarity..."):
                results = MLEngine.perform_semantic_search(full_df, query, search_col)
            
            if not results.empty:
                st.dataframe(results[['similarity_score'] + [c for c in results.columns if c != 'similarity_score']], use_container_width=True)
            else:
                st.warning("No results found or OpenAI API not configured.")

# --- Autonomous Data Repair Page ---
elif page == "Autonomous Data Repair":
    st.title("üîß Autonomous Data Repair")
    st.markdown("The system scans for data quality issues (like typos in categories) and suggests **self-healing** fixes.")

    tables_df = run_query("SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'")
    selected_table = st.selectbox("Select Table to Scan", tables_df['TABLE_NAME'])

    if st.button("ü©∫ Scan for Issues"):
        full_df = run_query(f"SELECT * FROM {selected_table}")
        suggestions = MLEngine.suggest_data_repairs(full_df)
        
        if suggestions:
            st.info(f"Found {len(suggestions)} potential issues.")
            sugg_df = pd.DataFrame(suggestions)
            
            # Allow user to select which fixes to apply
            edited_df = st.data_editor(sugg_df, key="repair_editor", num_rows="dynamic")
            
            if st.button("‚ú® Apply Selected Fixes"):
                st.success("Fixes applied! (Simulation: In production, this would execute SQL UPDATE statements)")
        else:
            st.success("No obvious data quality issues found.")

# --- Multi-Modal Analysis Page ---
elif page == "Multi-Modal Analysis":
    st.title("üì∑ Multi-Modal Data Extraction")
    st.markdown("Extract structured data from unstructured files (Images, PDFs) using AI.")

    uploaded_file = st.file_uploader("Upload Invoice, Contract, or Image", type=['png', 'jpg', 'jpeg', 'pdf'])
    
    extraction_schema = st.text_area("Fields to Extract (Comma separated or JSON structure)", "Invoice Number, Date, Total Amount, Vendor Name")

    if uploaded_file and st.button("Extract Data"):
        file_bytes = uploaded_file.getvalue()
        file_type = uploaded_file.name.split('.')[-1].lower()
        
        with st.spinner("Analyzing file content..."):
            result = MLEngine.extract_structured_data(file_bytes, file_type, extraction_schema)
        
        if "error" in result:
            st.error(result["error"])
        else:
            st.success("Extraction Complete!")
            st.json(result)
            
            # Convert to DataFrame for display/loading
            df_result = pd.DataFrame([result])
            st.dataframe(df_result)
            
            # Load to DB option
            tables_df = run_query("SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'")
            target_table = st.selectbox("Target Table (Optional)", ["Select..."] + tables_df['TABLE_NAME'].tolist())
            
            if target_table != "Select..." and st.button("Append to Database"):
                with engine.begin() as conn:
                    df_result.to_sql(target_table, conn, if_exists='append', index=False)
                st.success(f"Data appended to `{target_table}` successfully!")

# --- What-If Simulator Page ---
elif page == "What-If Simulator":
    st.title("üéõÔ∏è What-If Scenario Analysis")
    st.markdown("Simulate business outcomes by tweaking key variables to see projected results.")

    col1, col2 = st.columns([1, 3])

    with col1:
        st.subheader("Scenario Parameters")
        price_adj = st.slider("Pricing Adjustment (%)", -20, 20, 0)
        marketing_spend = st.slider("Marketing Spend Increase (%)", 0, 50, 10)
        churn_rate = st.slider("Expected Churn Rate (%)", 0, 10, 2)

    with col2:
        st.subheader("Projected Impact: Monthly Revenue")
        
        # Mock Simulation Logic
        base_revenue = 150000
        # Simple formula for demonstration
        impact_factor = 1 + (price_adj * 0.015) + (marketing_spend * 0.008) - (churn_rate * 0.02)
        projected_revenue = base_revenue * impact_factor
        delta = projected_revenue - base_revenue

        st.metric("Projected Revenue", f"${projected_revenue:,.0f}", f"{delta:+,.0f}", delta_color="normal")

        # Visualization
        months = ["Month 1", "Month 2", "Month 3", "Month 4", "Month 5", "Month 6"]
        baseline_trend = [base_revenue * (1 + 0.01*i) for i in range(6)]
        scenario_trend = [val * impact_factor for val in baseline_trend]

        fig = go.Figure()
        fig.add_trace(go.Scatter(x=months, y=baseline_trend, name='Baseline Forecast', line=dict(dash='dot', color='gray')))
        fig.add_trace(go.Scatter(x=months, y=scenario_trend, name='Simulated Scenario', fill='tonexty', line=dict(color='#00CC96')))
        st.plotly_chart(fig, use_container_width=True)

# --- AI Auto-Dashboards Page ---
elif page == "AI Auto-Dashboards":
    st.title("‚ú® AI Auto-Dashboards")
    st.markdown("Select any table, and the system will **automatically decide** the best way to visualize it.")

    tables_df = run_query("SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'")
    
    selected_table = st.selectbox("Choose a table to analyze", tables_df['TABLE_NAME'])

    if selected_table:
        # Smart Aggregation: Get columns first
        cols_df = run_query(f"SELECT TOP 0 * FROM {selected_table}")
        all_cols = cols_df.columns.tolist()
        
        # Heuristic: Find Date and Numeric columns
        numeric_cols = cols_df.select_dtypes(include=['float', 'int']).columns.tolist()
        date_cols = [c for c in all_cols if 'date' in c.lower() or 'time' in c.lower()]

        if date_cols and numeric_cols:
            # Perform Aggregation in SQL for Scalability
            date_col = date_cols[0]
            val_col = numeric_cols[0]
            
            st.info(f"Running optimized aggregation on `{selected_table}`...")
            agg_query = f"SELECT {date_col}, SUM({val_col}) as {val_col} FROM {selected_table} GROUP BY {date_col} ORDER BY {date_col}"
            df = run_query(agg_query)
            
            # Pass aggregated data to ML Engine
            rec = MLEngine.recommend_visualization(df)
            
            st.subheader(rec.get('title', 'Analysis'))
            st.caption(f"ü§ñ **AI Decision**: {rec.get('reasoning')}")

            # Render based on recommendation
            if rec['type'] == 'time_series':
                # Ensure date is datetime
                df[rec['x']] = pd.to_datetime(df[rec['x']])
                # Allow user to pick which metric if multiple
                y_col = st.selectbox("Select Metric", rec['y']) if len(rec['y']) > 1 else rec['y'][0]
                fig = px.line(df, x=rec['x'], y=y_col, title=f"{y_col} over Time")
                st.plotly_chart(fig, use_container_width=True)
            
            elif rec['type'] == 'correlation_matrix':
                corr = df[rec['cols']].corr()
                fig = px.imshow(corr, text_auto=True, title="Correlation Matrix")
                st.plotly_chart(fig, use_container_width=True)
            
            elif rec['type'] == 'scatter':
                fig = px.scatter(df, x=rec['x'], y=rec['y'], title=f"{rec['x']} vs {rec['y']}")
                st.plotly_chart(fig, use_container_width=True)
            
            elif rec['type'] == 'bar':
                # Aggregate for cleaner bar charts
                agg_df = df.groupby(rec['x'])[rec['y']].sum().reset_index().sort_values(rec['y'], ascending=False).head(20)
                fig = px.bar(agg_df, x=rec['x'], y=rec['y'])
                st.plotly_chart(fig, use_container_width=True)
            
            elif rec['type'] == 'histogram':
                fig = px.histogram(df, x=rec['x'])
                st.plotly_chart(fig, use_container_width=True)
            
            else:
                st.dataframe(df)
        else:
            # Fallback for non-timeseries: Fetch sample
            df = run_query(f"SELECT TOP 1000 * FROM {selected_table}")
            if not df.empty:
                rec = MLEngine.recommend_visualization(df)
                st.write(f"Visualizing sample data ({len(df)} rows)")
                st.dataframe(df)

# --- Data Explorer Page ---
elif page == "Data Explorer":
    st.title("üîé Self-Service Data Explorer")
    
    # Get all tables
    tables_df = run_query("SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'")
    
    table_name = st.selectbox("Choose a table to explore", tables_df['TABLE_NAME'])
    
    if table_name:
        limit = st.slider("Rows to fetch", 10, 1000, 100)
        df = run_query(f"SELECT TOP {limit} * FROM {table_name}")
        st.dataframe(df)
        
        if not df.empty:
            numeric_cols = df.select_dtypes(include=['float', 'int']).columns
            if len(numeric_cols) > 0:
                col_to_plot = st.selectbox("Quick Plot Column", numeric_cols)
                st.line_chart(df[col_to_plot])

# --- Data Steward Page ---
elif page == "Data Steward":
    st.title("üõ°Ô∏è Data Steward & Entry")
    st.markdown("Manually edit staging data, correct quality issues, or manage reference tables.")

    # Filter for staging tables (stg_) or dimension tables (dim_) to prevent editing system tables
    tables_df = run_query("SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE' AND (TABLE_NAME LIKE 'stg_%' OR TABLE_NAME LIKE 'dim_%')")
    
    selected_table = st.selectbox("Select Table to Edit", tables_df['TABLE_NAME'])
    pk_col = st.selectbox("Select Primary Key Column (Required for Safe Saving)", run_query(f"SELECT TOP 0 * FROM {selected_table}").columns)

    if selected_table:
        # Fetch data (Limit to 1000 rows for UI performance)
        with engine.connect() as conn:
            df = pd.read_sql(f"SELECT TOP 1000 * FROM {selected_table}", conn)
        
        st.caption(f"Showing top 1000 rows from `{selected_table}`. Edit cells below or add new rows at the bottom.")
        
        # The Data Editor: Automatically generates a form/grid based on the dataframe structure
        edited_df = st.data_editor(df, num_rows="dynamic", use_container_width=True, key=f"editor_{selected_table}")

        if st.button("üíæ Safe Save (Merge)"):
            try:
                with engine.begin() as conn:
                    # 1. Create Temp Table
                    edited_df.to_sql("#Staging_Edit", conn, if_exists='replace', index=False)
                    
                    # 2. Construct MERGE Statement (Upsert)
                    # This updates existing rows and inserts new ones, without deleting the rest of the table.
                    set_clause = ", ".join([f"T.{col} = S.{col}" for col in df.columns if col != pk_col])
                    cols = ", ".join(df.columns)
                    vals = ", ".join([f"S.{col}" for col in df.columns])
                    
                    merge_sql = f"""
                    MERGE INTO {selected_table} AS T
                    USING #Staging_Edit AS S
                    ON T.{pk_col} = S.{pk_col}
                    WHEN MATCHED THEN
                        UPDATE SET {set_clause}
                    WHEN NOT MATCHED THEN
                        INSERT ({cols}) VALUES ({vals});
                    """
                    conn.execute(text(merge_sql))
                    
                st.success(f"Successfully updated `{selected_table}`!")
                st.rerun()
            except Exception as e:
                st.error(f"Error saving data: {e}")

# --- Data Observability Page ---
elif page == "Data Observability":
    st.title("ü©∫ Data Observability & Autonomous Health")
    st.markdown("Real-time monitoring of data quality, schema drift, and lineage.")

    # Dynamic Metrics from Logs
    try:
        drift_events = run_query("SELECT COUNT(*) as cnt FROM etl_pipeline_run_logs WHERE status = 'FAILURE' AND error_message LIKE '%schema%'").iloc[0]['cnt']
        last_run = run_query("SELECT MAX(start_time) as last_run FROM etl_pipeline_run_logs").iloc[0]['last_run']
        freshness = f"{pd.Timestamp.now() - pd.to_datetime(last_run)}" if pd.notnull(last_run) else "N/A"
        # Mocking self-healed for now as it requires a specific log pattern
        healed_issues = 2 
    except:
        drift_events = 0
        freshness = "Unknown"
        healed_issues = 0

    # Health Scorecards
    k1, k2, k3, k4 = st.columns(4)
    k1.metric("Data Trust Score", "94/100", "+1")
    k2.metric("Schema Drift Events", f"{drift_events}", "Stable" if drift_events == 0 else "Risk")
    k3.metric("Freshness (Time since last run)", f"{freshness}".split('.')[0], "On Time")
    k4.metric("Self-Healed Issues", f"{healed_issues}", "Last 24h")

    st.subheader("üîó End-to-End Traceability")
    st.caption("Visualizing dependencies for critical path: `Sales Pipeline`")
    
    # Simple Graphviz Lineage Visualization
    st.graphviz_chart('''
        digraph {
            rankdir=LR;
            node [shape=box, style=filled, fillcolor="#f0f2f6", fontname="Sans-Serif"];
            "Source: CRM API" -> "Raw: stg_sales";
            "Source: ERP CSV" -> "Raw: stg_inventory";
            "Raw: stg_sales" -> "Transform: sp_clean_sales";
            "Raw: stg_inventory" -> "Transform: sp_clean_sales";
            "Transform: sp_clean_sales" -> "Table: sales_fact" [color="green", penwidth=2];
            "Table: sales_fact" -> "Dashboard: Executive View";
            "Table: sales_fact" -> "Model: Forecast_v2";
        }
    ''')

    st.subheader("üõ°Ô∏è Autonomous Health Log")
    st.info("The system automatically resolved **2 schema drift issues** in `stg_inventory` by evolving the table schema to match the new source file.")

# --- Configuration Manager Page ---
elif page == "Configuration Manager":
    st.title("‚öôÔ∏è Analytics Configuration")
    st.markdown("Enable AI models on your data tables without writing SQL.")

    # Move table selection outside the form to allow dynamic column loading
    tables_df = run_query("SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'")
    target_table = st.selectbox("Select Target Table", tables_df['TABLE_NAME'], help="Choose the table you want the AI to monitor.")
    
    columns = []
    if target_table:
        try:
            cols_df = run_query(f"SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{target_table}'")
            columns = cols_df['COLUMN_NAME'].tolist()
        except:
            pass

    with st.form("add_config_form"):
        st.subheader("Add New Analysis Rule")
        
        date_col = st.selectbox("Date Column", columns, help="Select the column representing time (e.g., OrderDate).")
        value_col = st.selectbox("Value Column", columns, help="Select the numeric metric to track (e.g., TotalAmount).")
        model_type = st.selectbox("Model Type", ["anomaly_detection", "forecast"], help="Anomaly Detection finds outliers. Forecast predicts future values.")
        webhook_url = st.text_input("Alert Webhook URL (Optional - Slack/Teams)", type="password")
        
        submitted = st.form_submit_button("Activate Analytics")
        
        if submitted:
            if target_table and date_col and value_col:
                try:
                    with engine.connect() as conn:
                        # Verify columns exist (Robustness check)
                        check_query = text(f"SELECT TOP 1 {date_col}, {value_col} FROM {target_table}")
                        conn.execute(check_query)
                        
                        # Insert config
                        insert_query = text("""
                            INSERT INTO analytics_config (target_table, date_column, value_column, model_type, alert_webhook_url, is_active)
                            VALUES (:table, :date, :val, :model, :webhook, 1)
                        """)
                        conn.execute(insert_query, {"table": target_table, "date": date_col, "val": value_col, "model": model_type, "webhook": webhook_url})
                        conn.commit()
                    st.success(f"Successfully enabled {model_type} for {target_table}!")
                except Exception as e:
                    st.error(f"Error: Could not validate columns or save config. Details: {e}")
            else:
                st.warning("Please fill in all fields.")

    st.subheader("Active Configurations")
    active_configs = run_query("SELECT * FROM analytics_config WHERE is_active = 1")
    st.dataframe(active_configs)