import streamlit as st
import pandas as pd
from sqlalchemy import create_engine, text
import os
from dotenv import load_dotenv
import plotly.express as px
import plotly.graph_objects as go

# Load environment variables
load_dotenv()

# Page Config
st.set_page_config(page_title="Modern Analytics Hub", page_icon="üìà", layout="wide")

# Database Connection
@st.cache_resource
def get_connection():
    db_connection_str = (
        f"mssql+pyodbc://{os.getenv('DB_USERNAME')}:{os.getenv('DB_PASSWORD')}@"
        f"{os.getenv('DB_SERVER')}/{os.getenv('DB_DATABASE')}?"
        f"driver={os.getenv('DB_DRIVER')}&TrustServerCertificate={os.getenv('DB_TRUST_SERVER_CERTIFICATE')}"
    )
    return create_engine(db_connection_str)

engine = get_connection()

# --- Sidebar ---
st.sidebar.title("Analytics & AI Hub")
page = st.sidebar.radio("Navigate", ["Dashboard", "Predictive Insights", "Data Explorer", "Configuration Manager"])

# --- Dashboard Page ---
if page == "Dashboard":
    st.title("üöÄ Executive Dashboard")
    st.markdown("Real-time overview of pipeline health and key metrics.")
    
    col1, col2, col3 = st.columns(3)
    
    with engine.connect() as conn:
        # Example metrics
        run_count = pd.read_sql("SELECT COUNT(*) as cnt FROM etl_pipeline_run_logs WHERE status = 'SUCCESS'", conn).iloc[0]['cnt']
        fail_count = pd.read_sql("SELECT COUNT(*) as cnt FROM etl_pipeline_run_logs WHERE status = 'FAILURE'", conn).iloc[0]['cnt']
        anomaly_count = pd.read_sql("SELECT COUNT(*) as cnt FROM analytics_predictions WHERE is_anomaly = 1", conn).iloc[0]['cnt']

    col1.metric("Total Successful Runs", run_count)
    col2.metric("Failed Runs", fail_count, delta_color="inverse")
    col3.metric("AI Detected Anomalies", anomaly_count, delta_color="inverse")

    st.subheader("Recent Activity")
    with engine.connect() as conn:
        history = pd.read_sql("SELECT TOP 10 pipeline_name, start_time, status FROM etl_pipeline_run_logs ORDER BY start_time DESC", conn)
    st.dataframe(history, use_container_width=True)

# --- Predictive Insights Page ---
elif page == "Predictive Insights":
    st.title("ü§ñ AI & Predictive Analytics")
    
    with engine.connect() as conn:
        tables = pd.read_sql("SELECT DISTINCT target_table FROM analytics_predictions", conn)
    
    selected_table = st.selectbox("Select Data Source", tables['target_table'])
    
    if selected_table:
        query = f"""
            SELECT prediction_date, actual_value, predicted_value, is_anomaly 
            FROM analytics_predictions 
            WHERE target_table = '{selected_table}'
            ORDER BY prediction_date
        """
        data = pd.read_sql(query, engine)
        
        # Visualization
        fig = go.Figure()
        
        # Actuals (if available in this table structure)
        if 'actual_value' in data.columns and data['actual_value'].notna().any():
            fig.add_trace(go.Scatter(x=data['prediction_date'], y=data['actual_value'], mode='lines', name='Actual Value'))
            
            # Anomalies
            anomalies = data[data['is_anomaly'] == 1]
            fig.add_trace(go.Scatter(x=anomalies['prediction_date'], y=anomalies['actual_value'], mode='markers', name='Anomaly', marker=dict(color='red', size=10)))

        # Forecasts
        forecasts = data[data['predicted_value'].notna()]
        if not forecasts.empty:
            fig.add_trace(go.Scatter(x=forecasts['prediction_date'], y=forecasts['predicted_value'], mode='lines', name='AI Forecast', line=dict(dash='dash')))

        st.plotly_chart(fig, use_container_width=True)
        
        st.info("üí° **AI Insight**: Anomalies detected in red. Dashed lines indicate future predictions generated by the ML Engine.")

# --- Data Explorer Page ---
elif page == "Data Explorer":
    st.title("üîé Self-Service Data Explorer")
    
    # Get all tables
    with engine.connect() as conn:
        # SQL Server specific query to get tables
        tables_df = pd.read_sql("SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'", conn)
    
    table_name = st.selectbox("Choose a table to explore", tables_df['TABLE_NAME'])
    
    if table_name:
        limit = st.slider("Rows to fetch", 10, 1000, 100)
        df = pd.read_sql(f"SELECT TOP {limit} * FROM {table_name}", engine)
        st.dataframe(df)
        
        if not df.empty:
            numeric_cols = df.select_dtypes(include=['float', 'int']).columns
            if len(numeric_cols) > 0:
                col_to_plot = st.selectbox("Quick Plot Column", numeric_cols)
                st.line_chart(df[col_to_plot])

# --- Configuration Manager Page ---
elif page == "Configuration Manager":
    st.title("‚öôÔ∏è Analytics Configuration")
    st.markdown("Enable AI models on your data tables without writing SQL.")

    with st.form("add_config_form"):
        st.subheader("Add New Analysis Rule")
        
        # Fetch available tables for dropdown
        with engine.connect() as conn:
            tables_df = pd.read_sql("SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE = 'BASE TABLE'", conn)
        
        target_table = st.selectbox("Target Table", tables_df['TABLE_NAME'])
        date_col = st.text_input("Date Column Name (e.g., sale_date)")
        value_col = st.text_input("Value Column Name (e.g., total_amount)")
        model_type = st.selectbox("Model Type", ["anomaly_detection", "forecast"])
        
        submitted = st.form_submit_button("Activate Analytics")
        
        if submitted:
            if target_table and date_col and value_col:
                try:
                    with engine.connect() as conn:
                        # Verify columns exist (Robustness check)
                        check_query = text(f"SELECT TOP 1 {date_col}, {value_col} FROM {target_table}")
                        conn.execute(check_query)
                        
                        # Insert config
                        insert_query = text("""
                            INSERT INTO analytics_config (target_table, date_column, value_column, model_type, is_active)
                            VALUES (:table, :date, :val, :model, 1)
                        """)
                        conn.execute(insert_query, {"table": target_table, "date": date_col, "val": value_col, "model": model_type})
                        conn.commit()
                    st.success(f"Successfully enabled {model_type} for {target_table}!")
                except Exception as e:
                    st.error(f"Error: Could not validate columns or save config. Details: {e}")
            else:
                st.warning("Please fill in all fields.")

    st.subheader("Active Configurations")
    with engine.connect() as conn:
        active_configs = pd.read_sql("SELECT * FROM analytics_config WHERE is_active = 1", conn)
    st.dataframe(active_configs)